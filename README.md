# ğŸ›’ KiVendTout - Architecture Data Engineering & IA

**Projet de Master 1 - Data Engineering & IA**  
**Sujet** : Architecture de donnÃ©es pour plateforme e-commerce (dÃ©tection fraude, BI temps rÃ©el, reconnaissance CNI)  
**Bloc** : RNCP40875 - Bloc 1 - Concevoir, dÃ©velopper et dÃ©ployer une architecture de donnÃ©es  
**Date** : FÃ©vrier 2026

---

## ğŸ“‹ Table des matiÃ¨res

- [Contexte du projet](#-contexte-du-projet)
- [Architecture technique](#-architecture-technique)
- [Stack technologique](#-stack-technologique)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [DÃ©marrage rapide](#-dÃ©marrage-rapide)
- [Services & AccÃ¨s](#-services--accÃ¨s)
- [Structure du projet](#-structure-du-projet)
- [Workflows](#-workflows)
- [Documentation](#-documentation)

---

## ğŸ¯ Contexte du projet

### ProblÃ©matiques mÃ©tier
KiVendTout, plateforme e-commerce en forte croissance, fait face Ã  :
- âœ… Augmentation des **fraudes aux paiements**
- âœ… DifficultÃ© Ã  **analyser le comportement utilisateurs**
- âœ… **Temps de rÃ©ponse longs** pour les Ã©quipes mÃ©tiers
- âœ… IncapacitÃ© Ã  **historiser les Ã©vÃ©nements** utilisateurs
- âœ… Nouvelle **contrainte lÃ©gale** : contrÃ´le d'identitÃ© pour ventes rÃ©glementÃ©es

### Objectifs du projet
1. Stocker les donnÃ©es critiques avec **intÃ©gritÃ© totale** (OLTP)
2. Exploiter les **Ã©vÃ©nements utilisateurs** en temps rÃ©el
3. Centraliser les donnÃ©es dans un **Data Lake** scalable
4. DÃ©tecter les **fraudes en temps rÃ©el** via stream processing
5. Exposer les donnÃ©es via **API standardisÃ©e**
6. RÃ©duire les **temps d'analyse BI**
7. Garantir **scalabilitÃ©** et **haute disponibilitÃ©**
8. Assurer **conformitÃ© RGPD** et sÃ©curitÃ©
9. AmÃ©liorer la **qualitÃ© des donnÃ©es**
10. DÃ©ployer un modÃ¨le **IA de reconnaissance de CNI**

---

## ğŸ—ï¸ Architecture technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SOURCES DE DONNÃ‰ES                        â”‚
â”‚     Web    â”‚   Mobile   â”‚   CRM   â”‚  Paiement  â”‚  Logistique   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚         â”‚          â”‚          â”‚            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                      â”‚   KAFKA     â”‚ Streaming (3 brokers HA)
                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
        â”‚   FLINK   â”‚  â”‚  MongoDB  â”‚  â”‚  MinIO  â”‚
        â”‚(DÃ©tection â”‚  â”‚  (Logs)   â”‚  â”‚(D. Lake)â”‚
        â”‚  fraude)  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â”‚
              â”‚                            â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚         PostgreSQL (OLTP)              â”‚
        â”‚   Clients â”‚ Commandes â”‚ Paiements      â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚  AIRFLOW  â”‚ Orchestration ETL/ELT
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚    dbt    â”‚ Transformations SQL + Tests
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚SUPERSETâ”‚ â”‚FastAPIâ”‚ â”‚TensorFlowâ”‚
â”‚  (BI)  â”‚ â”‚ (API) â”‚ â”‚   (IA)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚        â”‚        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  PROMETHEUS +   â”‚
     â”‚    GRAFANA      â”‚
     â”‚  (Monitoring)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Stack technologique

| Composant | Technologie | Version | RÃ´le |
|-----------|-------------|---------|------|
| **OLTP** | PostgreSQL | 15+ | Base relationnelle ACID |
| **NoSQL** | MongoDB | 7+ | Logs & Ã©vÃ©nements |
| **Data Lake** | MinIO + Parquet | Latest | Stockage massif S3-compatible |
| **Streaming** | Apache Kafka | 3.6+ | Message broker haute volumÃ©trie |
| **Processing** | Apache Flink | 1.18+ | Traitement temps rÃ©el |
| **Orchestration** | Apache Airflow | 2.8+ | Workflow ETL/ELT |
| **Transformation** | dbt | 1.7+ | SQL transformations |
| **IA/ML** | TensorFlow + OpenCV | 2.15+ | Reconnaissance CNI |
| **API** | FastAPI | 0.109+ | REST API + Swagger |
| **BI** | Apache Superset | 3.0+ | Dashboards interactifs |
| **Monitoring** | Prometheus + Grafana | Latest | MÃ©triques & alerting |
| **Quality** | Great Expectations | 0.18+ | Data validation |
| **Infra** | Docker Compose | Latest | Containerisation |

**Langage principal** : Python 3.10+  
**CoÃ»t total** : 0â‚¬ (100% open source)

---

## âœ… PrÃ©requis

### Logiciels requis
- âœ… **Docker Desktop** (installÃ©) - [TÃ©lÃ©charger](https://www.docker.com/products/docker-desktop)
- âœ… **Git** (installÃ© sur macOS par dÃ©faut)
- âœ… **Python 3.10+** - Installation : `brew install python@3.10`
- âš ï¸ **16 GB RAM minimum** (32 GB recommandÃ©)
- âš ï¸ **50 GB d'espace disque**

### VÃ©rification
```bash
docker --version          # Docker version 24.0+
docker compose version    # Docker Compose version v2.20+
python3 --version         # Python 3.10+
git --version            # git version 2.30+
```

---

## ğŸ“¦ Installation

### 1. Cloner le repository
```bash
git clone <votre-repo-git>
cd Patator
```

### 2. CrÃ©er l'environnement Python
```bash
# CrÃ©er un environnement virtuel
python3 -m venv venv

# Activer l'environnement
source venv/bin/activate  # macOS/Linux

# Installer les dÃ©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Configuration des variables d'environnement
```bash
# Copier le template
cp .env.example .env

# Ã‰diter avec vos valeurs
nano .env  # ou vim, code, etc.
```

---

## ğŸš€ DÃ©marrage rapide

### DÃ©marrer tous les services
```bash
# DÃ©marrer l'infrastructure complÃ¨te
docker compose up -d

# VÃ©rifier que tous les services sont UP
docker compose ps

# Voir les logs en temps rÃ©el
docker compose logs -f
```

### DÃ©marrer services individuellement
```bash
# Seulement la base de donnÃ©es
docker compose up -d postgres

# Seulement Kafka + Zookeeper
docker compose up -d zookeeper kafka-1 kafka-2 kafka-3

# Seulement le monitoring
docker compose up -d prometheus grafana
```

### ArrÃªter les services
```bash
# ArrÃªter tout
docker compose down

# ArrÃªter + supprimer les volumes (âš ï¸ perte de donnÃ©es)
docker compose down -v
```

---

## ğŸŒ Services & AccÃ¨s

Une fois les services dÃ©marrÃ©s :

| Service | URL | Identifiants | Description |
|---------|-----|--------------|-------------|
| **PostgreSQL** | `localhost:5432` | `postgres` / `postgres` | Base de donnÃ©es relationnelle |
| **MongoDB** | `localhost:27017` | `admin` / `admin` | Base NoSQL |
| **MinIO** | http://localhost:9001 | `minio` / `minio123` | Interface Data Lake |
| **Kafka UI** | http://localhost:8080 | - | Monitoring Kafka |
| **Airflow** | http://localhost:8081 | `airflow` / `airflow` | Orchestration |
| **Superset** | http://localhost:8088 | `admin` / `admin` | Business Intelligence |
| **FastAPI** | http://localhost:8000/docs | - | API Documentation (Swagger) |
| **Grafana** | http://localhost:3000 | `admin` / `admin` | Dashboards monitoring |
| **Prometheus** | http://localhost:9090 | - | MÃ©triques |

---

## ğŸ“ Structure du projet

```
Patator/
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ STACK_TECHNIQUE.md            # Documentation stack
â”œâ”€â”€ docker-compose.yml            # Orchestration services
â”œâ”€â”€ .env.example                  # Template variables d'environnement
â”œâ”€â”€ .gitignore                    # Fichiers Ã  ignorer
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”‚
â”œâ”€â”€ airflow/                      # Apache Airflow
â”‚   â”œâ”€â”€ dags/                     # Workflows ETL/ELT
â”‚   â”‚   â”œâ”€â”€ etl_postgres_to_lake.py
â”‚   â”‚   â”œâ”€â”€ etl_lake_to_warehouse.py
â”‚   â”‚   â””â”€â”€ data_quality_checks.py
â”‚   â”œâ”€â”€ plugins/                  # Plugins custom
â”‚   â””â”€â”€ config/                   # Configuration
â”‚
â”œâ”€â”€ api/                          # FastAPI
â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e API
â”‚   â”œâ”€â”€ routers/                  # Routes REST
â”‚   â”‚   â”œâ”€â”€ customers.py
â”‚   â”‚   â”œâ”€â”€ orders.py
â”‚   â”‚   â”œâ”€â”€ fraud.py
â”‚   â”‚   â””â”€â”€ identity.py
â”‚   â”œâ”€â”€ models/                   # ModÃ¨les Pydantic
â”‚   â”œâ”€â”€ schemas/                  # SchÃ©mas SQL
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ dbt/                          # dbt transformations
â”‚   â”œâ”€â”€ models/                   # ModÃ¨les SQL
â”‚   â”‚   â”œâ”€â”€ staging/              # Tables staging
â”‚   â”‚   â”œâ”€â”€ intermediate/         # Tables intermÃ©diaires
â”‚   â”‚   â””â”€â”€ mart/                 # Tables finales (DWH)
â”‚   â”œâ”€â”€ tests/                    # Tests data quality
â”‚   â”œâ”€â”€ macros/                   # Macros rÃ©utilisables
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ flink/                        # Apache Flink
â”‚   â”œâ”€â”€ jobs/                     # Jobs stream processing
â”‚   â”‚   â”œâ”€â”€ fraud_detection.py
â”‚   â”‚   â””â”€â”€ real_time_aggregations.py
â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ ml/                           # Machine Learning
â”‚   â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_fraud_model.ipynb
â”‚   â”‚   â””â”€â”€ 03_cni_recognition.ipynb
â”‚   â”œâ”€â”€ models/                   # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ data/                     # Datasets
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ train_cni_model.py
â”‚       â””â”€â”€ inference_cni.py
â”‚
â”œâ”€â”€ data/                         # DonnÃ©es locales
â”‚   â”œâ”€â”€ raw/                      # DonnÃ©es brutes
â”‚   â”œâ”€â”€ processed/                # DonnÃ©es traitÃ©es
â”‚   â””â”€â”€ external/                 # DonnÃ©es externes
â”‚
â”œâ”€â”€ database/                     # Scripts SQL
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”œâ”€â”€ init/
â”‚   â”‚   â”‚   â””â”€â”€ 01_create_schema.sql
â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â””â”€â”€ mongodb/
â”‚       â””â”€â”€ init/
â”‚
â”œâ”€â”€ kafka/                        # Kafka configuration
â”‚   â”œâ”€â”€ producers/                # Producteurs de messages
â”‚   â”‚   â”œâ”€â”€ web_events.py
â”‚   â”‚   â””â”€â”€ payment_events.py
â”‚   â””â”€â”€ consumers/                # Consommateurs
â”‚
â”œâ”€â”€ monitoring/                   # Monitoring & observabilitÃ©
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â””â”€â”€ alerting/
â”‚
â”œâ”€â”€ tests/                        # Tests
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ api/
â”‚   â””â”€â”€ runbook/
â”‚
â””â”€â”€ scripts/                      # Scripts utilitaires
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ generate_sample_data.py
    â””â”€â”€ run_tests.sh
```

---

## ğŸ”„ Workflows

### 1. Pipeline Batch (Quotidien)
```
PostgreSQL â†’ Airflow â†’ MinIO (Parquet) â†’ dbt â†’ PostgreSQL (DWH) â†’ Superset
```

### 2. Pipeline Streaming (Temps rÃ©el)
```
Sources â†’ Kafka â†’ Flink â†’ PostgreSQL/MongoDB â†’ FastAPI â†’ Alertes
```

### 3. Pipeline IA
```
Upload CNI â†’ FastAPI â†’ TensorFlow/OpenCV â†’ Validation â†’ PostgreSQL
```

---

## ğŸ“š Documentation

### Guides de dÃ©marrage
- [Guide PostgreSQL](docs/database/postgresql.md)
- [Guide Kafka](docs/streaming/kafka.md)
- [Guide Airflow](docs/orchestration/airflow.md)
- [Guide API](docs/api/fastapi.md)
- [Guide ML](docs/ml/model_training.md)

### Ressources externes
- [Documentation PostgreSQL](https://www.postgresql.org/docs/)
- [Documentation Kafka](https://kafka.apache.org/documentation/)
- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation FastAPI](https://fastapi.tiangolo.com/)

---

## ğŸ‘¥ Ã‰quipe

- **Ã‰tudiant 1** : [Nom] - [RÃ´le principal]
- **Ã‰tudiant 2** : [Nom] - [RÃ´le principal]

---

## ğŸ“ Licence

Projet acadÃ©mique - EFREI M1 Data Engineering & IA - 2026

---

## ğŸ†˜ Support

Pour toute question :
1. Consulter la [documentation](docs/)
2. VÃ©rifier les [issues GitHub](../../issues)
3. Contacter l'Ã©quipe

---

**DerniÃ¨re mise Ã  jour** : 3 fÃ©vrier 2026  
**Statut du projet** : ğŸš§ En dÃ©veloppement
